# gemma3-in-opi5plus

Utilizar la NPU (Neural Processing Unit) del chip RK3588 (que viene en la Orange Pi 5) con Ollama tiene un matiz técnico importante que debes conocer antes de empezar:Ollama oficial NO soporta la NPU de Rockchip nativamente.Si instalas Ollama de la forma estándar (curl ...), este correrá utilizando la CPU (y quizás la GPU Mali para algunas tareas), pero la NPU de 6 TOPS se quedará inactiva. La arquitectura de Rockchip requiere convertir los modelos al formato .rknn propietario.Para usar la NPU con modelos como Gemma 3 (o Gemma 2), tienes dos caminos: el camino Fácil (CPU rápida) y el camino Avanzado (NPU real con fork).Aquí te explico cómo lograrlo:1. Preparación del Entorno (Crítico)Para que cualquier cosa funcione bien en la Orange Pi 5, necesitas el sistema operativo correcto. No uses la imagen oficial de Orange Pi si puedes evitarlo; usa la versión optimizada por la comunidad.OS Recomendado: Ubuntu 22.04/24.04 de Joshua Riek. Es la versión más estable y con mejores drivers para la NPU.Verificar NPU: Asegúrate de que los drivers rknn estén activos.Bashsudo dmesg | grep -i rknpu
Deberías ver mensajes indicando que la NPU ha sido inicializada.2. La Solución Real: Usar un Fork o RKNN-LLMDado que Ollama no "habla" directamente con la NPU, la comunidad ha creado "puentes". La forma más cercana de tener una experiencia tipo Ollama pero usando la NPU es mediante RKNN-LLM.Opción A: ezrknn-llm (Contenedor Docker)Esta es la forma más limpia. ezrknn-llm es un proyecto que envuelve la complejidad de Rockchip en un contenedor y expone una API similar a la de OpenAI/Ollama.Instalar Docker:Bashcurl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
Descargar el modelo convertido (.rknn):Aquí está el truco: La NPU no lee archivos .gguf (que usa Ollama). Necesita archivos .rknn.Busca en HuggingFace repositorios como pelochus/ezrknn-llm-models o similar para ver si Gemma 3 ya ha sido convertido. Si Gemma 3 es muy reciente, es probable que solo encuentres Gemma 2-2b-it o Gemma 2-9b-it.Descarga el modelo .rknn correspondiente a tu RAM (4GB/8GB/16GB).Correr el contenedor:Bashdocker run -d --name rknn-llm \
  --privileged \
  -v /dev/bus/usb:/dev/bus/usb \
  -v $(pwd)/modelos:/models \
  -p 8080:8080 \
  pelochus/rknn-llm:latest \
  --model /models/gemma-3-2b-it.rknn
(Nota: Reemplaza el nombre del modelo por el archivo que descargaste).3. La Alternativa: Ollama en CPU (Muy capaz)Si no encuentras Gemma 3 convertido a formato .rknn (lo cual es muy probable si el modelo acaba de salir), tu mejor opción es correrlo en la CPU de la Orange Pi 5. El RK3588 es sorprendentemente rápido y puede correr modelos cuantizados (Q4_K_M) a velocidades decentes (4-8 tokens/s).Instalar Ollama estándar:Bashcurl -fsSL https://ollama.com/install.sh | sh
Correr Gemma 3:Bashollama run gemma3
(Si Gemma 3 aún no está en la librería oficial, usa ollama run gemma2).Tabla Comparativa de Rendimiento (Estimado en OPi5)MétodoHardwareModeloVelocidad Aprox.ComplejidadOllama StdCPU (Cortex-A76)GGUF (Q4)5 - 8 t/sMuy BajaRKNN-LLMNPU (6 TOPS)RKNN15 - 25 t/sAlta (requiere conversión)Resumen de Pasos para "Gemma 3" en NPUSi Gemma 3 es el requisito estricto y quieres NPU:Verifica la disponibilidad: Busca en HuggingFace "Gemma 3 RK3588" o "Gemma 3 RKNN".Si no existe el .rknn: Tendrás que convertirlo tú mismo. Esto implica:Usar una PC x86 (Linux) potente.Instalar el rknn-toolkit2 de Rockchip.Descargar el modelo original en formato ONNX o PyTorch.Ejecutar el script de conversión y cuantización a w8a8 (int8).Pasar el archivo resultante a tu Orange Pi.Mi recomendación:Si estás empezando, instala Ollama estándar y úsalo con la CPU. La Orange Pi 5 tiene potencia suficiente para chatear fluidamente con modelos pequeños (2B o 7B/9B cuantizados). Solo métete en el lío de la NPU (RKNN) si necesitas una latencia ultra baja para una aplicación de robótica o voz en tiempo real.